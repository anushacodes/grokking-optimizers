{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# An extensive comparison and visualisation of popular optimisation alogrithms for gradient descent"
      ],
      "metadata": {
        "id": "sgVLipP5ayX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n"
      ],
      "metadata": {
        "id": "ai3wMuijbH84"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example function (quadratic)\n",
        "\"\"\"\n",
        "\n",
        "def function(x):\n",
        "    return x**2 + 5*x + 3\n",
        "\n",
        "def function_grad(x):\n",
        "    return 2*x + 5\n",
        "\n",
        "other functions for testing:\n",
        "\n",
        "(cubic)\n",
        "def function(x):\n",
        "    return x**3 - 3*x**2 + 2\n",
        "\n",
        "def function_grad(x):\n",
        "    return 3*x**2 - 6*x\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def function(x):\n",
        "    return np.sin(3 * x) + 0.5 * x**2\n",
        "\n",
        "def function_grad(x):\n",
        "    return 3 * np.cos(3 * x) + x\n",
        "\n"
      ],
      "metadata": {
        "id": "2vNGdD_GbMWG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def create_3d_gif(history, filename):\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Define the x and y range for the function\n",
        "    x_min, x_max = min(history) - 2, max(history) + 2\n",
        "    x_vals = np.linspace(x_min, x_max, 100)\n",
        "    y_vals = np.linspace(x_min, x_max, 100)\n",
        "    X, Y = np.meshgrid(x_vals, y_vals)\n",
        "\n",
        "    # Define the function (assuming f(x, y) = f(x) + f(y))\n",
        "    Z = function(X) + function(Y)\n",
        "\n",
        "    # Scatter plot of optimization path\n",
        "    history_x = np.array(history)\n",
        "    history_y = np.array(history)  # Assuming single-variable optimization (x = y)\n",
        "    history_z = function(history_x) + function(history_y)\n",
        "\n",
        "    def update(i):\n",
        "        ax.clear()\n",
        "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
        "        ax.scatter(history_x[:i], history_y[:i], history_z[:i], color='red', s=40)\n",
        "\n",
        "        ax.set_xlabel(\"x\")\n",
        "        ax.set_ylabel(\"y\")\n",
        "        ax.set_zlabel(\"f(x, y)\")\n",
        "        ax.set_title(f\"Optimizer Path - Iteration {i+1}\")\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, update, frames=len(history), repeat=False)\n",
        "    ani.save(filename, writer=\"pillow\", fps=10)\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"3D GIF saved as {filename}\")\n"
      ],
      "metadata": {
        "id": "RXSyd4zzQ1gO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a function to create a gif of the training process\n",
        "\n"
      ],
      "metadata": {
        "id": "mPHJszTpbPo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "def create_gif(history, filename):\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "    x_min, x_max = min(history) - 10, max(history) + 10\n",
        "    x_vals = np.linspace(x_min, x_max, 500)\n",
        "    y_vals = function(x_vals)\n",
        "\n",
        "    y_min, y_max = np.min(y_vals), np.max(y_vals)\n",
        "\n",
        "    y_margin = (y_max - y_min) * 0.2  # 20% margin\n",
        "    y_min -= y_margin\n",
        "    y_max += y_margin\n",
        "\n",
        "    def update(i):\n",
        "        ax.clear()\n",
        "        ax.plot(x_vals, y_vals, label=\"Function f(x)\", color=\"blue\", linewidth=2)\n",
        "        ax.scatter(history[i], function(history[i]), color=\"red\", s=100, label=\"Current x\")\n",
        "\n",
        "        ax.set_xlim(x_min, x_max)\n",
        "        ax.set_ylim(y_min, y_max)\n",
        "\n",
        "        ax.set_xlabel(\"x\")\n",
        "        ax.set_ylabel(\"f(x)\")\n",
        "        ax.set_title(f\"Gradient Descent Progress (Iteration {i+1})\")\n",
        "        ax.legend()\n",
        "        ax.grid(True)\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, update, frames=len(history), repeat=False)\n",
        "    ani.save(filename, writer=\"pillow\", fps=10)\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"GIF saved as {filename}\")\n"
      ],
      "metadata": {
        "id": "GGEVxfoZfC3_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "\n",
        "  Let θ be the parameter vector and J(θ) be the objective function\n",
        "\n",
        "  - Update rule: θₜ₊₁ = θₜ - η∇J(θₜ)\n",
        "\n",
        "  - ∇J(θₜ) is the gradient of the objective function with respect to θ"
      ],
      "metadata": {
        "id": "4FN3f5qbjI3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(lr, initial_x, iters, eps = 1e-4):\n",
        "\n",
        "  \"\"\"\n",
        "    Parameters:\n",
        "    - lr: learning rate, which controls step size in each iteration\n",
        "    - initial_x: starting point for gradient descent\n",
        "    - iters: maximum number of iterations to run\n",
        "    - eps: convergence threshold. Stops if step size is smaller than this value\n",
        "\n",
        "    Returns:\n",
        "    - x: approximate local minimum found\n",
        "    - history: list of x values at each iteration\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "# initialising x at any given point (in practice, we want to randomise this)\n",
        "  x = initial_x\n",
        "  history = []\n",
        "\n",
        "  for i in range(iters):\n",
        "    gradient = function_grad(x)   # calcs gradient of a given point\n",
        "    x_new = x - lr * gradient     # x_new = x_old - n(∇f(x))\n",
        "    history.append(x_new)         # to store values of x for visualisation later\n",
        "\n",
        "    # checking for convergence\n",
        "    if abs(x_new - x) < eps:\n",
        "        print(f\"Converged after {i+1} iterations.\")\n",
        "        print(f\"Iteration {i+1}: x = {x_new}, f(x) = {function(x_new)}\")\n",
        "        return x_new, history     # returns results early if converged\n",
        "\n",
        "    x = x_new   # for next iter we update x_new -> x_old\n",
        "\n",
        "  # if the loop completes without convergence, returns the last computed x\n",
        "  print(f\"Iteration {i+1}: x = {x}, f(x) = {function(x)}\")\n",
        "\n",
        "  return x, history\n"
      ],
      "metadata": {
        "id": "1udxHmKDbMYm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, history = gradient_descent(0.1, 5, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cBzAGo1ydDaC",
        "outputId": "8b3c1c5b-c35f-49f0-e66c-ef3b17de9d9c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 31 iterations.\n",
            "Iteration 31: x = 1.4079621743825572, f(x) = 0.10814209088122684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"gradient_descent.gif\"\n",
        "\n",
        "create_gif(history, filename)\n",
        "print(\"GIF saved as 'gradient_descent.gif'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VquWk4Bpg5kF",
        "outputId": "4b228ffc-b7b5-4895-f36d-316120a9cdba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved as gradient_descent.gif\n",
            "GIF saved as 'gradient_descent.gif'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_3d_gif(history, \"gradient_descent_3d.gif\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDUoBNxfTC03",
        "outputId": "fa342519-8fa5-4271-a156-b59a9e6bab63"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3D GIF saved as gradient_descent_3d.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stochastic Gradient Descent\n",
        "\n",
        "It approximates the gradient using a small subset of data (or a single sample) in each iteration.\n",
        "\n",
        "  - Same update rule as standard gradient descent, but ∇Jᵢ(θₜ) is the gradient computed on a random sample i or a mini-batch.\n",
        "\n",
        "  - This introduces noise but allows for faster iterations and can help escape local minima.\n"
      ],
      "metadata": {
        "id": "GCMu7s38ie7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochasticGD(lr, initial_x, iters, eps = 1e-4):\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  Parameters: (same as previous)\n",
        "    - lr: learning rate\n",
        "    - initial_x: starting point for gradient descent\n",
        "    - iters: maximum number of iterations\n",
        "    - eps: convergence threshold\n",
        "\n",
        "    Returns:\n",
        "    - x: approximate local minimum found\n",
        "    - history: list of x values at each iteration\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  x = initial_x\n",
        "  history = []\n",
        "\n",
        "  for i in range(iters):\n",
        "      # to introduce randomness (simulate noisy gradients)\n",
        "      noise = np.random.normal(0, 0.1)  # small gaussian noise\n",
        "      gradient = function_grad(x) + noise\n",
        "\n",
        "      x_new = x - lr * gradient\n",
        "      history.append(x_new)\n",
        "\n",
        "      # check for convergence\n",
        "      if abs(x_new - x) < eps:\n",
        "          print(f\"Converged after {i+1} iterations.\")\n",
        "          print(f\"Iteration {i+1}: x = {x_new}, f(x) = {function(x_new)}\")\n",
        "          return x_new, history\n",
        "\n",
        "      x = x_new\n",
        "\n",
        "  print(f\"Iteration {iters}: x = {x}, f(x) = {function(x)}\")\n",
        "\n",
        "  return x, history"
      ],
      "metadata": {
        "id": "Eiat5-b7gJSL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_x, sgd_history = stochasticGD(lr = 0.1, initial_x = 5, iters = 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6sB2YeUjgae",
        "outputId": "5ea5726a-8256-42d1-aa79-e8a824e7a9e9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 200: x = -0.920674240127139, f(x) = -1.54038866584046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"sgd.gif\"\n",
        "\n",
        "create_gif(sgd_history, filename)\n",
        "print(\"GIF saved as 'sgd.gif'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RRxmmqKjhHy",
        "outputId": "21952df9-ac2b-48f3-ee17-5bd05a1727be"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved as sgd.gif\n",
            "GIF saved as 'sgd.gif'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_3d_gif(sgd_history, \"sgd_3d.gif\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A62SALwTJVc",
        "outputId": "82311068-86bb-4f77-8abb-78908c74828c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3D GIF saved as sgd_3d.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Gradient Descent\n"
      ],
      "metadata": {
        "id": "65wXyW-OkFR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(lr, initial_x, iters, batch_size, eps = 1e-4):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters (extra):\n",
        "        - batch_size: number of random samples used to approximate the gradient\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    x = initial_x\n",
        "    history = [x]\n",
        "\n",
        "    # computes batch gradient approximation by averaging gradients over batch_size samples\n",
        "    for i in range(iters):\n",
        "        batch_grad = np.mean([function_grad(x + np.random.uniform(-0.5, 0.5)) for _ in range(batch_size)])\n",
        "\n",
        "        # updates x based on the batch gradient and learning rate\n",
        "        x_new = x - lr * batch_grad\n",
        "\n",
        "        history.append(x_new)\n",
        "\n",
        "        if abs(x_new - x) < eps:\n",
        "            print(f\"Converged after {i+1} iterations.\")\n",
        "            print(f\"Final: x = {x_new}, f(x) = {function(x_new)}\")\n",
        "            return x_new, history\n",
        "\n",
        "        x = x_new\n",
        "\n",
        "    print(f\"Max iterations reached ({iters}).\")\n",
        "    print(f\"Final: x = {x}, f(x) = {function(x)}\")\n",
        "    return x, history\n",
        "\n"
      ],
      "metadata": {
        "id": "BaUtEcx8jqa-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_x, batch_history = batch_gradient_descent(lr = 0.02, initial_x = 5, iters = 201, batch_size = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEAdy4pvkMZW",
        "outputId": "9a6523a7-b7c4-4542-ff91-adf2b6770896"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max iterations reached (201).\n",
            "Final: x = 1.3019475376985266, f(x) = 0.15553795762244038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"batch_gd.gif\"\n",
        "\n",
        "create_gif(batch_history, filename)\n",
        "print(\"GIF saved as 'batch_gd.gif'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBpJRerTkN8t",
        "outputId": "0626de46-8941-405d-8880-ee8a86c19efe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved as batch_gd.gif\n",
            "GIF saved as 'batch_gd.gif'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ehzSGF7TQxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD with Momentum\n",
        "\n",
        "- Speeds up SGD by adding momentum in the relevant direction to overcome saddle points and local minima.\n",
        "\n",
        "- The idea is to add a fraction of the previous update to the current update, helping to accelerate convergence and reduce oscillation\n",
        "\n",
        "Formula:\n",
        "\n",
        "- It introduce a velocity vector vₜ = βvₜ₋₁ + η∇J(θₜ)\n",
        "\n",
        "- Update rule: θₜ₊₁ = θₜ - ηvₜ\n",
        "\n",
        "- Where β is the momentum coefficient (typically 0.9)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rVIwDb5PBV6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def function(x):\n",
        "    return x**2 + 3*x + 5\n",
        "\n",
        "def function_grad(x):\n",
        "    return 2*x + 3"
      ],
      "metadata": {
        "id": "H5TFwvyyCr_r"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd_with_momentum(lr, initial_x, iters, beta = 0.9, eps = 1e-4):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters:\n",
        "        - beta: Momentum coefficient (default 0.9)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    x = initial_x\n",
        "    v = 0  # initialize momentum term\n",
        "    history = [x]\n",
        "\n",
        "    for i in range(iters):\n",
        "        gradient = function_grad(x)\n",
        "        v = beta * v + lr * gradient  # update velocity term\n",
        "        x_new = x - lr * v\n",
        "\n",
        "        history.append(x_new)\n",
        "\n",
        "        # Convergence check\n",
        "        if abs(x_new - x) < eps:\n",
        "            print(f\"Converged after {i+1} iterations.\")\n",
        "            print(f\"Final: x = {x_new}, f(x) = {function(x_new)}\")\n",
        "            return x_new, history\n",
        "\n",
        "        x = x_new  # Update x for next iteration\n",
        "\n",
        "    print(f\"Max iterations reached ({iters}).\")\n",
        "    print(f\"Final: x = {x}, f(x) = {function(x)}\")\n",
        "    return x, history\n"
      ],
      "metadata": {
        "id": "rnpyUpzEknXP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_opt, sgd_mom_history = sgd_with_momentum(lr = 0.1, initial_x = -4, iters = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5IL-9QaJTmH",
        "outputId": "6c697479-9d9c-49d2-851c-0460819c4a44"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 93 iterations.\n",
            "Final: x = -1.5187757644204394, f(x) = 2.750352529329572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"sgd_momentum.gif\"\n",
        "create_gif(sgd_mom_history, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwBP3ASmFt-M",
        "outputId": "595b21ba-3f84-47c2-f584-51064de462b8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved as sgd_momentum.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaGrad\n",
        "\n",
        "- Adds to SGD by introducing learning rate as a parameter, gets modified according to previous gradients for each parameter\n",
        "\n",
        "- If the previous gradient is large, the learning rate gets smaller. However this causes a problem in later step as th elearning rate becomes too small as gradient keeps accumulating.\n",
        "\n",
        "- Useful for sparse data"
      ],
      "metadata": {
        "id": "pndzf94hCmlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def function(x):\n",
        "    return x**2 + 3*np.sin(x)\n",
        "\n",
        "def function_grad(x):\n",
        "    return 2*x + 3*np.cos(x)  # Derivative of f(x)\n"
      ],
      "metadata": {
        "id": "8hLUlYCRJDRy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AdaGrad(lr, initial_x, iters, eps = 1e-4):\n",
        "  g = 0    # running sum of squared gradients (lr)\n",
        "  x = initial_x\n",
        "  history = [x]\n",
        "  new_lr = 0\n",
        "\n",
        "  for i in range(iters):\n",
        "    gradient = function_grad(x)\n",
        "    g += gradient ** 2\n",
        "\n",
        "    new_lr = lr / (np.sqrt(g) + 1e-8)\n",
        "\n",
        "    x -= new_lr * gradient\n",
        "\n",
        "    history.append(x)\n",
        "\n",
        "    # Convergence check\n",
        "    if abs(gradient) < eps:\n",
        "        print(f\"Converged after {i+1} iterations.\")\n",
        "        print(f\"Final: x = {x}, f(x) = {function(x)}\")\n",
        "        return x, history\n",
        "\n",
        "  print(f\"Max iterations reached ({iters}).\")\n",
        "  print(f\"Final: x = {x}, f(x) = {function(x)}\")\n",
        "  return x, history\n"
      ],
      "metadata": {
        "id": "HNqrsTAeF6ya"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_adagrad, adagrad_history = AdaGrad(lr = 0.9, initial_x = 4, iters = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eaHexdfJm3z",
        "outputId": "e84eda6f-3ee2-4287-e35f-f3a82a5a59fc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 42 iterations.\n",
            "Final: x = -0.9148453218403361, f(x) = -1.5404628054370935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adagrad_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4h7kTyrKk9s",
        "outputId": "cf31e9b4-db05-404b-d3c0-a47f9cdfa5be"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4,\n",
              " 3.100000001490296,\n",
              " 2.6783416864882197,\n",
              " 2.350592220333358,\n",
              " 2.0509205542565296,\n",
              " 1.7544007708293532,\n",
              " 1.450173540477058,\n",
              " 1.1361543896087731,\n",
              " 0.8177474984277625,\n",
              " 0.5062208874883785,\n",
              " 0.21546880770747873,\n",
              " -0.04208298667488036,\n",
              " -0.258718338567969,\n",
              " -0.4325694880228168,\n",
              " -0.566660923554137,\n",
              " -0.6668815510744233,\n",
              " -0.7400239977779683,\n",
              " -0.7924821055783032,\n",
              " -0.8296390263365665,\n",
              " -0.8557273760429119,\n",
              " -0.8739320527041523,\n",
              " -0.8865812475141349,\n",
              " -0.8953443273471251,\n",
              " -0.901402780628205,\n",
              " -0.9055854471745448,\n",
              " -0.908470286768775,\n",
              " -0.9104586629924687,\n",
              " -0.911828518094478,\n",
              " -0.9127719540149515,\n",
              " -0.913421567402206,\n",
              " -0.9138687984804151,\n",
              " -0.9141766659924723,\n",
              " -0.9143885824524957,\n",
              " -0.9145344451167865,\n",
              " -0.914634839376132,\n",
              " -0.9147039370602219,\n",
              " -0.9147514936969965,\n",
              " -0.9147842242979077,\n",
              " -0.9148067507876938,\n",
              " -0.9148222543248486,\n",
              " -0.9148329243760266,\n",
              " -0.9148402678428031,\n",
              " -0.9148453218403361]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "\n",
        "filename = \"adagrad.gif\"\n",
        "create_gif(adagrad_history, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEvNl1cBIYdh",
        "outputId": "0cea652b-3691-4b70-8927-b250b52b6729"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved as adagrad.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WbH4xxqRNHX6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adagrad experiments"
      ],
      "metadata": {
        "id": "-T-TiUS9NGmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def AdaGrad(lr, initial_x, iters, eps = 1e-5):\n",
        "  g = 0    # running sum of squared gradients (lr)\n",
        "  x = initial_x\n",
        "  history = [x]\n",
        "  new_lr = 0\n",
        "\n",
        "  for i in range(iters):\n",
        "    gradient = function_grad(x)\n",
        "    g += gradient ** 2\n",
        "\n",
        "    new_lr = lr / (np.sqrt(g) + 1e-1)\n",
        "\n",
        "    x -= new_lr * gradient\n",
        "\n",
        "    history.append(x)\n",
        "\n",
        "    # Convergence check\n",
        "    if abs(gradient) < eps:\n",
        "        print(f\"Converged after {i+1} iterations.\")\n",
        "        print(f\"Final: x = {x}, f(x) = {function(x)}\")\n",
        "        return x, history\n",
        "\n",
        "  print(f\"Max iterations reached ({iters}).\")\n",
        "  print(f\"Final: x = {x}, f(x) = {function(x)}\")\n",
        "  return x, history\n"
      ],
      "metadata": {
        "id": "zBVNCDj_NHjF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_adagrad_exp, adagrad_exp_history = AdaGrad(lr = 0.3, initial_x = 4, iters = 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca4264e-96c8-4c9d-c6ff-6d8c74b27142",
        "id": "d6QJgSGdNHjG"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max iterations reached (200).\n",
            "Final: x = -0.9121287137042742, f(x) = -1.5404465263496046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adagrad_history[:-1:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d9b42f4-941e-48c6-cd79-621ca5b1029f",
        "id": "xORBhrddNHjG"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4,\n",
              " 1.7544007708293532,\n",
              " 0.21546880770747873,\n",
              " -0.6668815510744233,\n",
              " -0.8739320527041523,\n",
              " -0.908470286768775,\n",
              " -0.9138687984804151,\n",
              " -0.9147039370602219,\n",
              " -0.9148329243760266]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "\n",
        "filename = \"adagrad_exp.gif\"\n",
        "create_gif(adagrad_exp_history, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10bdbd88-d25e-468f-c364-9ea1bd84c16a",
        "id": "G2jDQlwrNHjH"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved as adagrad_exp.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_3d_gif(adagrad_exp_history, \"optimizer_path_3d.gif\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "450-v93SQ4jy",
        "outputId": "3f3da96d-c74b-4e20-8e87-451d253ee939"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3D GIF saved as optimizer_path_3d.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSProp"
      ],
      "metadata": {
        "id": "reoJF-qAEytE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSProp(lr, initial_x, iters, eps = 1e-5, beta = 0.9):     # new term - beta introduced\n",
        "  g = 0    # running sum of squared gradients (lr)\n",
        "  x = initial_x\n",
        "  history = [x]\n",
        "  new_lr = 0\n",
        "\n",
        "  for i in range(iters):\n",
        "    gradient = function_grad(x)\n",
        "    # uppdated moving average of squared gradients\n",
        "    g = beta * g +(1 - beta) * gradient ** 2\n",
        "\n",
        "    new_lr = lr / (np.sqrt(g) + 1e-5) # adaptive learning rate\n",
        "\n",
        "    x -= new_lr * gradient\n",
        "\n",
        "    history.append(x)\n",
        "\n",
        "    # convergence check\n",
        "    if abs(gradient) < eps:\n",
        "        print(f\"Converged after {i+1} iterations.\")\n",
        "        print(f\"Final: x = {x}, f(x) = {function(x)}\")\n",
        "        return x, history\n",
        "\n",
        "  print(f\"Max iterations reached ({iters}).\")\n",
        "  print(f\"Final: x = {x}, f(x) = {function(x)}\")\n",
        "  return x, history\n"
      ],
      "metadata": {
        "id": "egnvAtaYE1Vi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_rmsprop, rmsprop_history = AdaGrad(lr = 0.01, initial_x = 4, iters = 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59cbff82-ad16-47e7-f35c-1bb36f2cfc03",
        "id": "rQ8LjTNMQE6h"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max iterations reached (200).\n",
            "Final: x = 3.741470279797083, f(x) = 12.304975445450939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "\n",
        "filename = \"rmsprop.gif\"\n",
        "create_gif(rmsprop_history, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a7224b-126b-4699-81af-d4cabed9a67f",
        "id": "vfJ3N2OmQE6i"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved as rmsprop.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adam Optimiser"
      ],
      "metadata": {
        "id": "HWD8p4e3E1-B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VujhHspxQ2zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mao6jJGRE4AS"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}